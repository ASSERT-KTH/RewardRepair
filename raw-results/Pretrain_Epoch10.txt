Epoch: 10, Loss:  0.6639928817749023
Epoch: 10, Loss:  0.6414912939071655
Epoch: 10, Loss:  0.7047352194786072
Epoch: 10, Loss:  0.22967016696929932
Epoch: 10, Loss:  0.4909868836402893
Epoch: 10, Loss:  0.6241415143013
Epoch: 10, Loss:  0.6849523186683655
Epoch: 10, Loss:  0.3821762800216675
Epoch: 10, Loss:  0.5874668955802917
Epoch: 10, Loss:  0.39088839292526245
Epoch: 10, Loss:  0.8151013851165771
Epoch: 10, Loss:  0.25752031803131104
Epoch: 10, Loss:  0.8935465216636658
Epoch: 10, Loss:  0.5741558074951172
Epoch: 10, Loss:  0.23955020308494568
Epoch: 10, Loss:  0.2404622882604599
Epoch: 10, Loss:  0.4480056166648865
Epoch: 10, Loss:  0.471280962228775
Epoch: 10, Loss:  0.6789777874946594
Epoch: 10, Loss:  0.7359225749969482
Epoch: 10, Loss:  0.7327092289924622
Epoch: 10, Loss:  0.25369927287101746
Epoch: 10, Loss:  0.2603120505809784
Validating on valid dataset *********: 10
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Total Loss:  32.38908126205206/205